% 
% 
% Interaction and Introspection with Tangible Augmented Objects

# {data-background="./img/title.jpg"}


## {data-background="./img/screen-wonderland.png"}
<div class="notes">
- Digital wonderland at fingers' reach
</div>


## {data-background="./img/screen-prisoners.png"}
<div class="notes">
- But cannot touch it
</div>


<!-- Attention -->
## {data-background="./img/sur-fake-5-crop.jpg"}
<div class="notes">
- Devices became portable
- But require our whole attention
- Either IN the screen our OUT in the world
</div>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<div class="copyright" style="right:0px;color:rgb(255,255,255);">Source: Antoine Geiger's SUR-FAKE</div>


---

### A **Humane** Representation of Thought
<img src="./img/ascent-of-man.jpg" height="300px">
<div class="footnote" style="position: fixed; bottom:-150px; left: 175px;">[@Victor2014]</div>
<div class="notes">
- Technology constrain our bodies
- Only eye and fingers working
- Bodies have been neglected
- Tip of finger interaction is limiting. Not humane.
- Evolution made it so that we think *with all our senses*
</div>


## {data-background="./img/conclusion-real-world.jpg"}
### Real World
<div class="notes">
We **know** how it works 
</div>


## {data-transition="fade"}
<img src="./img/intro-mug.png" height="600px" style="box-shadow: none; background: none;">


## {data-transition="fade"}
<img src="./img/intro-mug-affordance.png" height="600px" style="box-shadow: none; background: none;">

---

### Tangible User Interfaces
<img src="./img/radical-atoms.png" height="500px">

<div class="footnote">[@Ishii1997]<br>[@Ishii2012]</div>


## Augmented Objects
<img src="./img/intro-mug.png" height="500px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/intro-mug-augmented.png" height="500px">
<div class="notes">
- For example, we can augment a normal mug with different functions:
    + Displaying remaining steeping time
    + Displaying temperature of liquid inside
    + Handle turns green when everything is OK
</div>

---

### Augmented Reality
<img src="./img/intro-ar-video-see-through.png">
<div class="notes">
- Traditional way is to use video see-through
- Can also uses head mounted display
- However: requires hardware for user
</div>

---

### *Spatial* Augmented Reality {style="vertical-align: center;"}
Uses projectors or screens *in the environments* to display information spatially related to this environment
<div class="footnote" style="position: fixed; bottom:-300px; left: 100px;">[@Raskar1998a]<br>[@Raskar2001a]</div>

<div class="notes">
- SAR instead uses projector or screens *in the environment*
- Link with Ubicomp
</div>


<!-- SAR -->
## {data-transition="fade"}
<img src="./img/intro-mug-sar.png">

<div class="notes">
- Example mug from before
- Normal mug + projector...
</div>


## {data-transition="fade"}
<img src="./img/intro-mug-sar-augmented.png">

<div class="notes">
- Creates augmented mug
</div>

---

### Augmented Objects
<img src="./img/sar-clock.jpg" height="400px">
<img src="./img/teegi-inverse-model.jpg" height="400px">

<div class="notes">
- It can also be used to create augmented objects
- In these two pictures, *white* physical objects
</div>

---


### How does it work?


<!-- TODO: Declutter images -->
## {data-background="./img/sar-pipeline.png"}

---

### Augmented Objects
<img src="./img/augmented-object-input-output.png" height="600px">
<div class="notes">
My objective is to move towards a real-world experience without having to remove technology.
Twofold:
- By including real world elements with our use of technology
- Using technology as a way to reflect on ourselves
</div>

---

### Interaction
How can we *interact* with digital content hosted on physical objects?

<img src="./img/augmented-object-interaction.png" height="400px">

---

### Direct Interaction
<img src="./img/dynamic-shader-lamps.jpg" height="400px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 175px;">[@Bandyopadhyay2001]</div>

---

### Direct Interaction
<img src="./img/physical-virtual-tools.jpg" height="400px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/miragetable.jpg" height="400px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 100px;">[@Marner2009]<br>[@Benko2012]</div>

---

### Direct Vs. Indirect
<div class="notes">
- Show pointing on real object using direct touch
- Tangible interaction is sometimes not enough
- So we want to use digital tools
- Such tools are often operated on screen
- What works great on screen is indirect interaction such as the mouse
</div>


## {data-background="./img/cursar-teaser.png"}

<div class="notes">
- We focus on the creation of *hybrid environments*
- Desktop computers are still good platforms for content creation
</div>

---

### Interaction
<img src="./img/cursar-teaser.jpg" height="325px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/tports-video-teaser.png" height="325px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 100px;">[@Gervais2015]<br>Gervais, R., et al., 2016. Tangible Viewports: Getting out of flatland in desktop environments, in: TEI’16.</div>

<div class="notes">
The evaluation of the use of 2D pointing devices – mouse and
graphics tablet – in a pointing task in a SAR context compared
to a screen condition.

The design, implementation and evaluation of a system enabling
the interaction between a typical desktop computer environ-
ment – traditional screens, mouse and keyboard – with tangible
augmented objects, considering an object design scenario as a
main thread.
</div>

---

### Introspection
How can we use augmented objects to reveal hidden information about *our own selves*?

<img src="./img/augmented-object-introspection.png" height="400px">


---

### Introspection
<img src="./img/mind-mirror.gif" height="400px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 0px;">[@Mercier-Ganady2014]</div>

---

### Introspection
<img src="./img/bodyviz.jpg" height="400px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 50px;">[@Norooz2015]</div>

---

### Introspection
<img src="./img/teegi-teaser.jpg" width="600px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/tobe-coherence.jpg" width="600px">
<div class="footnote" style="position: fixed; bottom:-100px; left: 200px;">Frey, J., Gervais, R., et al., 2014. Teegi: Tangible EEG interface, in: UIST’14.<br>Gervais, R., Frey, J., et al., 2016. TOBE: Tangible Out-of-Body Experience, in: TEI’16.
</div>

<!-- 
### What is it good for?

---

### Pros
- Anchored in the *real world*
- User is free
- Scales well 
- Collaboration 

---

### Cons
- Projection surface
- Shadows
- Complexity
- *Interaction*
<div class="notes">
Complexity: Calibration with multiple devices (esp. /w multi-proj setup)
</div>
 -->


# Interaction





# Pointing in SAR {data-background="./img/cursar-cube-pointer-drawing.png"}


## {data-transition="fade"}
### Standard way of pointing
<img src="./img/cursar-virtual-pointing.png" height="500px">
<p class="fragment">Now what happens if...</p>


## {data-transition="fade"}
### Removing the screen
<img src="./img/cursar-virtual-pointing-no-screen.png" height="500px">
<p class="fragment">Does pointing still works without a screen?</p>


## Study

---

### Questions
Differences between *SCREEN* and *SAR* conditions for pointing?

Does pointing in SAR follow Fitts' law?


## {data-transition="fade"}
### Pointing technique
<img src="./img/cursar-virtual-pointing.png" height="500px">


## {data-transition="fade"}
### Pointing technique
<img src="./img/cursar-virtual-pointing-window.png" height="500px">

---

### Apparatus
<img src="./img/cursar-setup.png" width="950px" style="background:none; border:none; box-shadow:none;">

<div class="notes">
- A: Circle-shaped cursor that follows the geometry of the real world
- B: Plane onto which cursor is mapped
    + In SAR, plane is virtual
    + In SCREEN condition, we use a wooden panel to create a screen there
- C: Guide displayed on the table to help know where the cursor is located
- D: Position of the user is known
- E: Projector
    + Augment real cube in SAR condition
    + Projects a virtual cube in SCREEN condition
</div>

---

### SCREEN vs SAR
<img src="./img/cursar-screen.jpg" width="450px">
<img src="./img/cursar-sar.jpg" width="450px">

<div class="notes">
- On the left it is a simulated version of what is seen on the right
- Comparison of the view in both conditions
- The view of the cube is the same
    + In SCREEN condition, note the virtual table is aligned with real table
</div>

---

### Scene
<img src="./img/cursar-scene.jpg" width="700px">

<div class="notes">
- Scene was changing between trials
- Cube alone in different orientation
- Cube *and* a more complex shape
</div>

---

### Procedures
<video data-autoplay controls class="stretch" src="./img/video.mp4"></video>

<div class="notes">
1. Position cursor in starting zone
1. Zone changes from red to green
1. Target appears
1. User go click on target
1. Comes back to starting zone
</div>

---

### Design
<img src="./img/cursar-study-design.png" height="650px" style="-webkit-filter: none; filter: none;">

<div class="notes">
- Inefficiency: deviation from the most optimal path
</div>

---

### Participants
>- 16 participants
>- Familiar with mice
>- Little experience with graphics tablets
>- No experience with SAR systems




## Results

---

### Time 
Users were *11% faster* using a screen vs SAR

<div class="notes">
- Screen faster than SAR by 11%
- Drop of performance not so important: still usable
- Screen probably provide context for interaction
- No dead spaces in midair for SCREEN condition
</div>
---

### Inefficiency
<img src="./img/cursar-heatmap.png" height="500px" style="-webkit-filter: none; filter: none;">

<div class="notes">
- Input modality significant effect.
- Tablet is *less* efficient than mouse
- Explained by experience with mice vs graphics tablet
- See heatmap figure to show example of this
</div>

---

### Fitts' law
<img src="./img/cursar-fitts-eq.png" height="500px" style="-webkit-filter: none; filter: none;">
<div class="footnote" style="position: fixed; bottom:-50px; left: 225px;">[@MacKenzie1992]</div>

<div class="notes">
- MT: Movement Time
- ID: Index of Difficulty
- D: Projected target distance in virtual screen
- W: Perceived target size
- We modeled the movement time with a linear regression.

Note: $R^2=0.8479$
</div>





# <!-- Hybrid Workspaces --> {data-background="./img/tports-hybrid-workspaces.png"}


## {data-background="./img/tports-video-teaser.png" data-transition="fade"}

<div class="notes">
- We continued our investigation varying the different way to leverage the screen context
- Here, we keep the screen context for interaction and interact with a physical object when it is located *in front of the screen*
</div>


## {data-background="./img/tports-video-teaser-highlight.png" data-transition="fade"}
<div class="notes">
- Focus on hybrid environment
</div>

---

### Metaphor
<img src="./img/tports-cursor-application.png" height="500px">
<div class="notes">
Working on a physical object becomes the same thing as working on a virtual version in a window
</div>

---

### Pointing technique
<img src="./img/tports-cursor.png" height="500px">
<div class="notes">
- Same as with CurSAR
- Screen is tracked
- Behavior of cursor is coherent only for operator
- Cursor is visible by *everyone*
</div>


## {data-background-video="./img/tports.mp4"}
<!-- <video controls src="./img/tports.mp4"></video> -->


## Applications

---

### Painting / Progamming
<img src="./img/tports-direct-painting.jpg" height="500px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/tports-screen-vvvv.jpg" height="500px">

---

### Objects in Sync
<img src="./img/tports-synchro-sync.jpg" height="500px">
<span style="display:inline-block; width: 50px;"></span>
<img src="./img/tports-synchro-heatmap.jpg" height="500px">

---

### Objects in Sync
<img src="./img/tports-dataviz.jpg" height="500px">

---

### Text Annotations
<img src="./img/tports-text.jpg" height="500px">


## {data-background="./img/tports-video-teaser-bounded.png"}


## {data-background="./img/tports-video-teaser-unbounded.png"}

---

### Towards the Desk as a Working Space

---

### Direct-Indirect Combination
<img src="./img/conclusion-direct-indirect-combination-direct.png" height="400px">
<img src="./img/conclusion-direct-indirect-combination-indirect.png" height="400px">

---

### Improvised Working Surfaces
<img src="./img/conclusion-improvised-working-surfaces-1.png" height="250px">
<img src="./img/conclusion-improvised-working-surfaces-2.png" height="250px">


<img src="./img/conclusion-improvised-working-surfaces-3.png" height="250px">
<img src="./img/conclusion-improvised-working-surfaces-4.png" height="250px">





# Introspection {data-background="./img/tobe-coherence-wo-title.png"}
<div class="notes">
How to use these augmented objects to better know ourselves and others better?
</div>

---

### Collaboration
<img src="./img/teegi-jeremy.jpg" height="500px">




# {data-background="./img/teegi-big-head-whats-inside.png"}
<div class="notes">
- Do you know what is happening in your brain right now?
- Brain is interesting because except for the nagging little voice that is always there, we have no idea of what is currently going on in there
</div>

---

### EEG
<img src="./img/gtec_headset.jpg" height="500px">
<div class="notes">
- EEG: ElectroEncephaloGraphy
- Measures electrical activity at the surface of the scalp
- Can be used for
  - Active control (BCIs): thinking about hand movements or imagining mental rotations
  - Passive measurements
</div>



## {data-background="./img/teegi-eeg-tools.png"}
<div class="notes">
- Current tools are complex and difficult to approach
</div>



## {data-background="./img/teegi-big-head-whats-inside-no-question.png"}
### Objectives
- What can EEG measure?
- How does the brain work?




## {data-background="./img/teegi-alison.jpg"}
<div class="notes">
- let users explore their *own* brain
- in *real-time*
</div>


## <!-- Video teaser --> {data-background-video="./img/teegi.mp4"}


## <!-- Tangible filter control --> {data-background="./img/teegi-mini-teegis-photo.jpg"}
<div class="notes">
- Users were able to see different brain activity
- Filters were controlled using mini-teegis
</div>

---

### Filters
<img src="./img/teegi-filters.png" height="500px">

---

### Features

---

### Synchronized Blinks
<video controls data-autoplay loop src="./img/teegi_blinks.mp4" height=500px></video>

---

### Inverse Model
<img src="./img/teegi-inverse-model.jpg" height="500px">

---

### Exposing the Signal Processing
<img src="./img/teegi-signal-table.jpg" height="500px">

---

### Installation
<img src="./img/teegi-setup-drawing.png" height="600px">

---

### Exploratory Study
<img src="./img/teegi-questions.jpg" height="600px">

---

### Teegi Disco







# Going Further {style="color: #eee;" data-background="./img/tobe-tablet-window.jpg"}


---

### Inner Mirror
<img src="./img/inner-beauty.png" height="500px">

---

### Building your own mirror

---

### 
<img src="./img/tobe-pipeline.png" height="600px">

## <!-- Tobe --> {data-background-video="./img/tobe.mp4"}
<!-- <video controls class="stretch" src="./img/tobe.mp4"></video> -->



## Metrics {data-background="./img/tobe-gui.jpg"}

---

### Sensors & Signal Processing
<img src="./img/tobe-sensor-ecg.jpg" width="30%">
<img src="./img/tobe-sensor-breathing.jpg" width="30%">

<img src="./img/tobe-sensor-eda.jpg" width="30%">
<img src="./img/tobe-sensor-openbci.jpg" width="30%">

<div class="notes">
- ECG
- Breathing
- EDA
- EEG
- EOG
</div>

---

### 3D Printing
<img src="./img/tobe-potato.jpg" height="500px">



## {data-background="./img/tobe-tablet-window.jpg"}
### Feedback {style="color: #eee;"}

---

### Setup
<img src="./img/tobe-capscience-setup.png" height="600px">


## {data-background="./img/tobe-animator.jpg"}
### Customization {style="color: #eee;"}



## {data-background="./img/tobe-coherence.jpg"}
### Use Case {style="color: #eee;"}




# In Summary


## Contributions
<img src="./img/augmented-object-input-output.png" height="500px">


## Interaction
<img src="./img/conclusion-cursar.jpg" height="400px">
<span style="display:inline-block; width: 100px;"></span>
<img src="./img/conclusion-tports.jpg" height="400px">


## Introspection
<img src="./img/teegi-inverse-model.jpg" height="400px">
<span style="display:inline-block; width: 100px;"></span>
<img src="./img/conclusion-tobe.jpg" height="400px">





# <!-- What's next? --> {data-background="./img/scanning-the-horizon.jpg"}


## <!-- The Real World --> {data-background="./img/conclusion-real-world.jpg"}
<div class="notes">
I think that we should not have to choose between being *here* and going to wonderland.
</div>


## <!-- Being / Doing --> {data-background="./img/being-and-doing.png"}

---

### Designing for Calmness and Wellbeing
<div class="notes">
- How can we use technology to create calm experiences?
</div>

---

### Undivided Attention
<div class="notes">
- Trying to go towards technology that does not *require* our undivided attention
</div>



## {data-background="./img/tandem.jpg"}


## {data-background="./img/camping.jpg"}


# Thank You {data-background="./img/logos.png"}
renaudgervais.github.io

<small>These slides are available online at <br> renaudgervais.github.io/presentations</small>

# 

---

### Results
<img src="./img/table.png" width="950px" style="-webkit-filter: none; filter: none;">